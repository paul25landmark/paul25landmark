
ps -ef | grep kube 
kill <process id>

#xcrun error;
xcode-select --install #run this on your cmd line


1;19;00 1st KT session accessing bastion host
uname -a (run in cli)
Amazon-vpc-cni-k8s
docker images | grep precisely
kubectl taint node <nodname> key=value:NoSchedule (taint and untaint node)
kubectl (-n namespace) delete pod #to kill a pod the namespace is optional 

In Terraform , you can fetch data source by tags, by using the filter arg reference e.g
data "aws_vpc" "tuts" {
  filter {
    name = "tag:KeyName" #the Key=Name 
    value = ["ValueName"] #the Value=Value
  }
}


9090 Prom default
3000 Grafana def

echo $HISTFILE
Then cat the file 
cat /Users/Paul.Aje/.zsh_history

API automation #read
Web application Firewall WAF #read
Terraform cloud #read
diff btw efs and s3 #read
do a post mortem #to do

To do: 
-Document Nginx ingress upgrade /
-Document eks worker nodes creation.( monitoring and LIbigdata) /
-Document Addition of nodeSelectors 

Configuring slack notification in Terraform:
e.g
module "notify_slack" {
  source  = "terraform-aws-modules/notify-slack/aws"
  version = "~> 4.0"

  sns_topic_name = "slack-topic"

  slack_webhook_url = "https://hooks.slack.com/services/AAA/BBB/CCC"
  slack_channel     = "aws-notification"
  slack_username    = "reporter"
}
# Get the slack_webhook_url from the  top of the channel, click integrations, add workflow, and you should be able to add it 
------------------------------------------------------------------
Error: Error acquiring the state lock
│ 
│ Error message: resource temporarily unavailable
│ Lock Info:
│   ID:        bf072d8d-b170-1ae5-6cd4-94c1e8859ebf
│   Path:      .state/terraform.state
│   Operation: OperationTypePlan
│   Who:       Paul.Aje@PaulAje-Pro.local
│   Version:   1.1.8
│   Created:   2022-05-20 19:06:52.018578 +0000 UTC
│   Info:      
│ 
│ 
│ Terraform acquires a state lock to protect the state from being written
│ by multiple users at the same time. Please resolve the issue above and try
│ again. For most commands, you can disable locking with the "-lock=false"
│ flag, but this is not recommended.
╵solution:
terraform force-unlock -force 9db590f1-b6fe-c5f2-2678-8804f089deba
------------------------------------------------------------------

Issues:
Connecting to redis cluster from the pod
we exec into pod, 
update and install redis-cli with --tls option
then we use the token provided from the variables in gitlab and pass in the command 

to get auth-token: #run this command 
curl --location -v --request POST 'https://auth-dev.cloud.precisely.services/auth/realms/Precisely/protocol/openid-connect/token?workspaceId=10001qa' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data-urlencode 'client_id=OIDC-DIS-CustomerAccountManagement' \
--data-urlencode 'grant_type=password' \
--data-urlencode 'scope=openid offline_access' \
--data-urlencode 'username=ams_user_1@mailinator.com' \
--data-urlencode 'password=Password@1234'

Documentation:
- To Run a pipeline from your local:
go to gitalb, select the project
create a token and give it all access 
clone the repo
go to your local, and cd into the folder/
press cmd+shift+. to display hidden files
go back to gitlab, select Infrastructure on the left hand side and click on terraform
under the stage you want to deploy in, selct the 3 dots under actions and click on "Download json"
(this shows the state of that env = very critical)
then go back and select "copy Tf init cmd"
copy and paste the line that says "export GITLAB_ACCESS_TOKEN=<YOUR-ACCESS-TOKEN>" with your created token inserted
then copy and paste the remaining part of the cmd in your cmd line (see below example)
terraform init \
    -backend-config="address=https://gitlab.com/api/v4/projects/36196838/terraform/state/backup-dev" \
    -backend-config="lock_address=https://gitlab.com/api/v4/projects/36196838/terraform/state/backup-dev/lock" \
    -backend-config="unlock_address=https://gitlab.com/api/v4/projects/36196838/terraform/state/backup-dev/lock" \
    -backend-config="username=aj.paem" \
    -backend-config="password=$GITLAB_ACCESS_TOKEN" \
    -backend-config="lock_method=POST" \
    -backend-config="unlock_method=DELETE" \
    -backend-config="retry_wait_min=5"
after this is done, run the tf destroy cmd with the variables inputed manually. (e.g below) 
terraform destroy -auto-approve -var-file="env-config/dr/terraform.tfvars" -var="access_key=AKIAXM453GYZKL5MGDLN" -var="secret_key=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" -var="region=us-west-2"
#this will destroy the resources created without triggring the pipelin.
#Reason for doing this is if the build isn't successful during the apply stage, you can destroy it from your local.
#However, if build waS successful, then create a terraform_desrtoy branch  

collection:
a group of APIs


------------------------------------------------------------------

# configuring eks cluster and adding context to your local env 

cd ~
press cmd+shift+. so you can see hidden files 
touch .aws
login to console, create IAM user, get access key and secret
touch config and enter the details below;
# [pr-cloud-plt-dev-qa]
# region = us-east-2
# output = json
# [pr-cloud-plt-stg-prd]
# region = us-east-1
# output = json
# [default]
# region = us-east-2
# output = json
touch credentials and enter the details below with keys and secret;
# [pr-cloud-plt-dev-qa]
# aws_access_key_id=*****
# aws_secret_access_key=*****
# [pr-cloud-plt-stg-prd]
# aws_access_key_id=*****
# aws_secret_access_key=*****
# [default]
# aws_access_key_id=*****
# aws_secret_access_key=*****
after that, do the following on the cli;
# export AWS_PROFILE=pr-cloud-plt-dev-qa
# export AWS_PROFILE=pr-cloud-plt-stg-prd
then, to confirm, pass this;
# aws sts get-caller-identity
kubectl config current-context
Create the context for your user and env
# aws eks --region us-east-2 update-kubeconfig --name dev-cloud-platform-eks
# aws eks --region us-east-2 update-kubeconfig --name qa-cloud-platform-eks
# aws eks --region us-east-1 update-kubeconfig --name stg-cloud-platform-eks
# aws eks --region us-east-1 update-kubeconfig --name prd-cloud-platform-eks
To see the context, enter the below;
#cat ~/.kube/config
then to view current context do;
#brew install kubectx
#kubectx to view the context you are working on
to change to particulare context
#kubectx arn:aws:eks:us-east-2:508747789874:cluster/dev-cloud-platform-eks
------------------------------------------------------------------

deleting job/services from env 
# Deployment, configmaps, services, ingress, secret, custom Resources>scaled objects

commands:
kubectl exec -it podname 

stress.sh: (make cpu utilization to 100%)
sudo amazon-linux-extras install epel -y
sudo yum install stress -y
then run stress -c 4

bootstrap userdata script:
#!/bin/bash
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
EC2_AVAIL_ZONE=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)
echo "<h1>Hello World from $(hostname -f) in AZ $EC2_AVAIL_ZONE </h1>" > /var/www/html/index.html

http://54.193.30.68/ - west1
35.85.143.32 - west2
------------------------------------------------------------------
#issues with gitlab login - this error; 
use this cmd; git clone https://<GITLAB-ACCESS-TOKEN>@<gitlab repo>
when prompted for passwd, enter <GITLAB_ACCESS_TOKEN>
